#
# Parse grm_error csv file generated by Chuan
#
#
import re

import pandas as pd


def get_Id(s):
    return re.match('.+\_(\d+)$', s)[1]


def get_Set(s):
    return re.match('(.+)\_\d+$', s)[1]

# set_Id,ofto_error_count,Spell,Article_a_an_error_count,sva_error_count,RuleRepeatedLinkWords_error_count,Verbform_error_count,noun_error_count,Article_error_count,AdjAdv_error_count,Wordform_error_count,VerbCollocation_error_count
grm_error_file = '../data/processed/call_error_count.csv'

def load_df(grm_error_file):
    df = pd.read_csv(grm_error_file, sep=',', encoding="utf-8")
    df['Id'] = df.apply(lambda x: get_Id(x['set_Id']) , axis=1)
    df['Set'] = df.apply(lambda x: get_Set(x['set_Id']) , axis=1)
    df.drop(['set_Id'], axis=1)
    return df


grm_error_file = '../data/processed/grammar/call_error_count_dec2017.csv' # contain 2018
df_1 = load_df(grm_error_file)
grm_error_file = '../data/processed/grammar/call_error_count.csv'  # re-run using 2017 ASR
df_2 = load_df(grm_error_file)
grm_error_file = '../data/processed/grammar/y17test_y18train.csv'  # On 2017 test adn 2018 train ASR outputs.
df_3 = load_df(grm_error_file)
grm_error_file = '../data/processed/grammar/y17train.csv'  # On 2017 test adn 2018 train ASR outputs.
df_4 = load_df(grm_error_file)
grm_error_file = '../data/processed/grammar/2018test_text_asr.csv'  # On 2018 test text and LLS ASR.
df_5 = load_df(grm_error_file)
grm_error_file = '../data/processed/grammar/gec_trans.csv'  # using trans.
df_6 = load_df(grm_error_file)


set_dict = {'scst2_training_data_A_text': 'df18_A_train',
            'scst2_training_data_B_text': 'df18_B_train',
            'scst2_training_data_C_text': 'df18_C_train',
            }
# based on data, dump error count features to different csv
for name, group in df_1.groupby('Set'):
    if name in set_dict.keys():
        csv_out = '../data/processed/' + set_dict[name] + '_grmerror.csv'
        print(csv_out)
        # group.drop(['set_Id', 'Set'], axis=1).to_csv(csv_out, encoding='utf-8', index=False)


set_dict = {'2017_test_withASR': 'df17_test',
            'scst1_trainingData_textTask': 'df17_train'
            }
# based on data, dump error count features to different csv
for name, group in df_2.groupby('Set'):
    if name in set_dict.keys():
        csv_out = '../data/processed/' + set_dict[name] + '_grmerror.csv'
        print(csv_out)
        # group.drop(['set_Id', 'Set'], axis=1).to_csv(csv_out, encoding='utf-8', index=False)

set_dict = {'2017_test_asr': 'df17_test_asr',
            '2018_train_asr': 'df18_train_asr'
            }
# based on data, dump error count features to different csv
for name, group in df_3.groupby('Set'):
    if name in set_dict.keys():
        csv_out = '../data/processed/' + set_dict[name] + '_grmerror.csv'
        print(csv_out)
        # group.drop(['set_Id', 'Set'], axis=1).to_csv(csv_out, encoding='utf-8', index=False)

set_dict = {'2017_train_asr': 'df17_train_asr'
            }
# based on data, dump error count features to different csv
for name, group in df_4.groupby('Set'):
    if name in set_dict.keys():
        csv_out = '../data/processed/' + set_dict[name] + '_grmerror.csv'
        print(csv_out)
        # group.drop(['set_Id', 'Set'], axis=1).to_csv(csv_out, encoding='utf-8', index=False)

set_dict = {'2018_testset-lma-0.5-20.': 'df18_test_asr',
            'scst2_testDataText': 'df18_test_text'
            }
# based on data, dump error count features to different csv
for name, group in df_5.groupby('Set'):
    if name in set_dict.keys():
        csv_out = '../data/processed/' + set_dict[name] + '_grmerror.csv'
        print(csv_out)
        # group.drop(['set_Id', 'Set'], axis=1).to_csv(csv_out, encoding='utf-8', index=False)

# GEC features from trans (3/7/2018)
set_dict = {
    'scst1_trainingData_textTask': 'df17_train_tran',
    'scst1_testData_annotated': 'df17_test_tran',
    'scst2_training_data_A_text': 'df18_A_train_tran',
    'scst2_training_data_B_text': 'df18_B_train_tran',
    'scst2_training_data_C_text': 'df18_C_train_tran',
    'train_rec': 'df18_test_tran'
}
# based on data, dump error count features to different csv
for name, group in df_6.groupby('Set'):
    if name in set_dict.keys():
        csv_out = '../data/processed/' + set_dict[name] + '_grmerror.csv'
        print(csv_out)
        group.drop(['set_Id', 'Set'], axis=1).to_csv(csv_out, encoding='utf-8', index=False)

